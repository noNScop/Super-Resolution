{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c439266c-1db4-45a7-9d7b-d64b524f38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a07ba7-e52b-4a98-b664-23be24b522f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff3c18b-6055-46eb-abaa-7e6eaedbe81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HR_train_paths = sorted(glob.glob(\"./data/DIV2K_train_HR/*.png\"))\n",
    "X2_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X2/*.png\"))\n",
    "X4_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X4/*.png\"))\n",
    "X8_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X8/*.png\"))\n",
    "X16_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X16/*.png\"))\n",
    "X32_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X32/*.png\"))\n",
    "X64_train_paths = sorted(glob.glob(\"./data/DIV2K_train_LR_bicubic/X64/*.png\"))\n",
    "\n",
    "HR_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_HR/*.png\"))\n",
    "X2_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X2/*.png\"))\n",
    "X4_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X4/*.png\"))\n",
    "X8_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X8/*.png\"))\n",
    "X16_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X16/*.png\"))\n",
    "X32_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X32/*.png\"))\n",
    "X64_valid_paths = sorted(glob.glob(\"./data/DIV2K_valid_LR_bicubic/X64/*.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58afb16-d84a-415c-957c-2d8eef775b0b",
   "metadata": {},
   "source": [
    "### FSRCNN - Fast Super-Resolution Convolutional Neural Network\n",
    "The architecture below is described in this paper: [Accelerating the Super-Resolution Convolutional Neural Network](https://arxiv.org/abs/1608.00367)\n",
    "\n",
    "I derived the formula for padding and absolute padding of the transposed convolution from this formula (which can be found in the [pytorch docs](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)):\n",
    "$$\n",
    "H_{\\text{out}} = (H_{\\text{in}} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n",
    "$$\n",
    "\n",
    "Thanks to it the output size image is exactly ```n x larger``` than the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c8165d-53f5-4349-b5c3-d5afae7c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, d: int, s: int, m: int, n: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d: feature dimension\n",
    "            s: shrinking dimension\n",
    "            m: mapping layers\n",
    "            n: scaling factor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            self._conv(3, d, 5),\n",
    "            self._conv(d, s, 1)\n",
    "        )\n",
    "\n",
    "        for _ in range(m):\n",
    "            self.model.append(self._conv(s, s, 3))\n",
    "\n",
    "        self.model.append(self._conv(s, d, 1))\n",
    "\n",
    "        # Ensure the output image is exactly n times bigger than the input\n",
    "        if n <= 9:\n",
    "            padding = (9 - n + 1) // 2\n",
    "            output_padding = (9 - n) % 2\n",
    "        else:\n",
    "            for i in range(n):\n",
    "                padding = i - n + 9\n",
    "                if padding % 2 == 0 and padding >= 0:\n",
    "                    output_padding = i\n",
    "                    break\n",
    "        \n",
    "        self.model.append(nn.ConvTranspose2d(d, 3, 9, stride=n, padding=padding, output_padding=output_padding))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _conv(self, ni, nf, ks):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ni, nf, ks, padding='same'),\n",
    "            nn.PReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d068037-4a90-4cae-830f-15cd9b719a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsr_cnn = FSRCNN(56, 12, 4, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a382b6aa-d5db-4f62-a826-ecc0e143e9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FSRCNN                                   [32, 3, 256, 256]         --\n",
       "├─Sequential: 1-1                        [32, 3, 256, 256]         --\n",
       "│    └─Sequential: 2-1                   [32, 56, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-1                  [32, 56, 128, 128]        4,256\n",
       "│    │    └─PReLU: 3-2                   [32, 56, 128, 128]        1\n",
       "│    └─Sequential: 2-2                   [32, 12, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-3                  [32, 12, 128, 128]        684\n",
       "│    │    └─PReLU: 3-4                   [32, 12, 128, 128]        1\n",
       "│    └─Sequential: 2-3                   [32, 12, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-5                  [32, 12, 128, 128]        1,308\n",
       "│    │    └─PReLU: 3-6                   [32, 12, 128, 128]        1\n",
       "│    └─Sequential: 2-4                   [32, 12, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-7                  [32, 12, 128, 128]        1,308\n",
       "│    │    └─PReLU: 3-8                   [32, 12, 128, 128]        1\n",
       "│    └─Sequential: 2-5                   [32, 12, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-9                  [32, 12, 128, 128]        1,308\n",
       "│    │    └─PReLU: 3-10                  [32, 12, 128, 128]        1\n",
       "│    └─Sequential: 2-6                   [32, 12, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-11                 [32, 12, 128, 128]        1,308\n",
       "│    │    └─PReLU: 3-12                  [32, 12, 128, 128]        1\n",
       "│    └─Sequential: 2-7                   [32, 56, 128, 128]        --\n",
       "│    │    └─Conv2d: 3-13                 [32, 56, 128, 128]        728\n",
       "│    │    └─PReLU: 3-14                  [32, 56, 128, 128]        1\n",
       "│    └─ConvTranspose2d: 2-8              [32, 3, 256, 256]         13,611\n",
       "==========================================================================================\n",
       "Total params: 24,518\n",
       "Trainable params: 24,518\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 34.26\n",
       "==========================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 1493.17\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 1499.56\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(fsr_cnn, (32, 3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21d899b-ab55-4f36-bda4-9e8e9b8511fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, target_paths: list[str], scale: int, crop_size: int=128):\n",
    "        self.target = target_paths\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "\n",
    "        # make sure that the scaling is possible\n",
    "        assert self.scale % 2 == 0 and self.scale <= crop_size\n",
    "        \n",
    "        self.transforms = v2.Compose([\n",
    "            v2.PILToTensor(),\n",
    "            v2.Lambda(lambda x: x/255.0)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target = Image.open(self.target[idx])\n",
    "        target = self.random_crop(target, self.crop_size)\n",
    "        inp = target.resize((target.width//self.scale, target.height//self.scale), Image.BICUBIC)\n",
    "        return self.transforms(inp), self.transforms(target)\n",
    "\n",
    "    def random_crop(self, img, size):\n",
    "        w, h = img.size\n",
    "        if w < size or h < size:\n",
    "            img = img.resize((size, size), Image.BICUBIC)  # Resize if image is too small\n",
    "            \n",
    "        x = random.randint(0, w - size)\n",
    "        y = random.randint(0, h - size)\n",
    "        return img.crop((x, y, x + size, y + size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a2c008-2e29-4fc6-a403-c82cdc70a2a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (4224303332.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "scales = [2, 4, 8, 16, 32, 64]\n",
    "\n",
    "for scale in tqdm(scales):\n",
    "    train_ds = SRDataset(HR_train_paths, scale)\n",
    "    valid_ds = SRDataset(HR_valid_paths, scale)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=10)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=False, num_workers=10)\n",
    "\n",
    "    model = FSRCNN(56, 12, 4, scale).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in tqdm(range(10), desc=\"Epochs\", leave=False):\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c71b7d3-e80d-4809-8366-285f289e804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, optimizer, scheduler, loss_fn, device, accuracy):\n",
    "    avg_accuracy = 0\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Necessary for dataloader to work with tqdm without errors, tqdm interferes with dataloader workers shutdown process,\n",
    "    # therefore I separated them\n",
    "    dl_iterator = iter(dataloader)\n",
    "    for _ in tqdm(range(len(dataloader)), desc=\"Training\", leave=False):\n",
    "        batch, target = next(dl_iterator)\n",
    "        batch, target = batch.to(device), target.to(device)\n",
    "        \n",
    "        logits = model(batch)\n",
    "        loss = loss_fn(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        avg_accuracy += accuracy(logits, target).item()\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "    avg_accuracy /= len(dataloader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def valid_step(model, dataloader, loss_fn, device, accuracy):\n",
    "    avg_accuracy = 0\n",
    "    avg_loss = 0\n",
    "    model.eval()\n",
    "\n",
    "    # Necessary for dataloader to work with tqdm without errors\n",
    "    dl_iterator = iter(dataloader)\n",
    "    with torch.inference_mode():\n",
    "        for _ in tqdm(range(len(dataloader)), desc=\"Validation\", leave=False):\n",
    "            batch, target = next(dl_iterator)\n",
    "            batch, target = batch.to(device), target.to(device)\n",
    "            \n",
    "            logits = model(batch)\n",
    "            loss = loss_fn(logits, target)\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            avg_accuracy += accuracy(logits, target).item()\n",
    "\n",
    "    avg_loss /= len(dataloader)\n",
    "    avg_accuracy /= len(dataloader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def train(model, train_dl, valid_dl, optimizer, loss_fn, epochs):\n",
    "    psnr = PeakSignalNoiseRatio()\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=2.0)\n",
    "    lpips = LearnedPerceptualImagePatchSimilarity().to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        train_loss, train_PSNR, SSIM = train_step(\n",
    "            model,\n",
    "            train_dl,\n",
    "            optimizer,\n",
    "            loss_fn\n",
    "        )\n",
    "\n",
    "        valid_loss, valid_PSNR, valid_SSIM = valid_step(\n",
    "            model,\n",
    "            valid_dl,\n",
    "            loss_fn\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_PSNR: {train_PSNR:.4f} | \"\n",
    "            f\"train_SSIM: {train_SSIM:.4f} | \"\n",
    "            f\"valid_loss: {valid_loss:.4f} | \"\n",
    "            f\"valid_PSNR: {valid_PSNR:.4f} | \"\n",
    "            f\"valid_SSIM: {valid_SSIM:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c28415-48f2-475f-bdcd-ffa49b7ea6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SR_venv",
   "language": "python",
   "name": "sr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
